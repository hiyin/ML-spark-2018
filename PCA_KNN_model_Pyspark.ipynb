{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>19</td><td>application_1527585229587_0033</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-hdiclu.ecvp4y2oanouxadossn44nhmmg.px.internal.cloudapp.net:8088/proxy/application_1527585229587_0033/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn4-hdiclu.ecvp4y2oanouxadossn44nhmmg.px.internal.cloudapp.net:30060/node/containerlogs/container_1527585229587_0033_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{u'executorCores': 6, u'numExecutors': 6, u'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>19</td><td>application_1527585229587_0033</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-hdiclu.ecvp4y2oanouxadossn44nhmmg.px.internal.cloudapp.net:8088/proxy/application_1527585229587_0033/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn4-hdiclu.ecvp4y2oanouxadossn44nhmmg.px.internal.cloudapp.net:30060/node/containerlogs/container_1527585229587_0033_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"executorCores\": 6, \"numExecutors\":6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().set(\"spark.executor.instances\", \"8\").setAppName('appName').setMaster('spark://localhost:7077')\n",
    "# Make sure one instance is running at once\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "# Create spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDF(path):\n",
    "    # rename column '_c0' to 'label'\n",
    "    f = spark.read.format('csv').options(header='false', inferSchema='true').load(path)\n",
    "    data = f.withColumnRenamed(\"_c0\",\"label\")\n",
    "    return data\n",
    "\n",
    "def generatePCA(trainingset, PCA_k):\n",
    "    transformer = VectorAssembler(inputCols=['_c%d' % i for i in range(1,785)],\n",
    "                              outputCol=\"features\")\n",
    "    standardizer = StandardScaler(withMean=True, withStd=True,\n",
    "                              inputCol='features',\n",
    "                              outputCol='std_features')\n",
    "    indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_idx\")\n",
    "    pca = PCA(k=PCA_k, inputCol=\"std_features\", outputCol=\"pca\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[transformer, standardizer, indexer, pca])\n",
    "    model = pipeline.fit(trainingset)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = createDF('wasb://hdinsight2-2018-05-28t14-28-17-337z@ryanstorage10.blob.core.windows.net/Train-label-28x28.csv')\n",
    "#test = createDF('wasb://hdinsight2-2018-05-28t14-28-17-337z@ryanstorage10.blob.core.windows.net/Test-label-28x28.csv')\n",
    "train = createDF(\"/Users/dyin/Desktop/Cloudass2-master/Train-label-28x28.csv\")\n",
    "test = createDF(\"/Users/dyin/Desktop/Cloudass2-master/Test-label-28x28.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating model\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe\n",
    "\n",
    "\n",
    "# Generate model\n",
    "print(\"Generating model\")\n",
    "model = generatePCA(train, 50)\n",
    "\n",
    "# Train model\n",
    "train_result = model.transform(train)\n",
    "test_result = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check trained results\n",
    "print(train_result.take(2))\n",
    "print(test_result.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform full data back to rdd\n",
    "train_result_rdd = train_result.select([\"label\", \"pca\"]).rdd\n",
    "test_result_rdd = test_result.select([\"label\", \"pca\"]).rdd\n",
    "\n",
    "#print(train_result_sample.take(1))\n",
    "#x = train_result_sample.mapPartitions(calculateDistance(train_result_sample, test_result_sample, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train_result_rdd.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this section to use portional data rdd\n",
    "# Take small sample from data: set variables to full data if not using parts of rdd for flexible computation\n",
    "test_x_rdd = sc.parallelize([ row['pca'] for row in test_result_rdd.take(1000) ])\n",
    "test_y_rdd = sc.parallelize([ row['label'] for row in test_result_rdd.take(1000) ])\n",
    "train_x_rdd = sc.parallelize([ row['pca'] for row in train_result_rdd.take(1000) ])\n",
    "train_y_rdd = sc.parallelize([ row['label'] for row in train_result_rdd.take(1000) ])\n",
    "\n",
    "print(train_x.take(1))\n",
    "print(test_x.take(1))\n",
    "print(np.array(train_x.collect())[1,:])\n",
    "print(np.array(test_x.collect())[1,:])\n",
    "#print(np.array(test_y.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this section to use full data rdd\n",
    "\n",
    "test_x_rdd = sc.parallelize([ row['pca'] for row in test_result_rdd.collect() ])\n",
    "test_y_rdd = sc.parallelize([ row['label'] for row in test_result_rdd.collect() ])\n",
    "train_x_rdd = sc.parallelize([ row['pca'] for row in train_result_rdd.collect() ])\n",
    "train_y_rdd = sc.parallelize([ row['label'] for row in train_result_rdd.collect() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core algorithms\n",
    "\n",
    "def calculateDistance(test_rdd, train_rdd):\n",
    "    #num_test = len(test)\n",
    "    #num_train = len(train)\n",
    "    num_test = test_rdd.count()\n",
    "    num_train = train_rdd.count()\n",
    "    # transform into array\n",
    "    test = np.array(test_rdd.collect())\n",
    "    train = np.array(train_rdd.collect())\n",
    "    dists = np.zeros((num_test, num_train))\n",
    "    for i in range(num_test):\n",
    "        test_i  = test[i, :]\n",
    "    #print(test_i)\n",
    "    #dists[i, :] = np.sqrt(np.power(train_ - test_i, 2).sum(axis=1))\n",
    "        dists[i, :] = np.sqrt(np.sum(np.power(train - test_i, 2), 1))\n",
    "    return dists\n",
    "#print(dists)  \n",
    "\n",
    "def predictLabels(dists, train_y_rdd, k):\n",
    "    train_y = np.array(train_y_rdd.collect())\n",
    "    num_test = dists.shape[0]\n",
    "    y_pred = np.zeros(num_test, dtype=int)\n",
    "    for i in range(num_test):\n",
    "        #A list of length k storing the labels of the k nearest neighbors to the ith test point.\n",
    "        closest_y = []\n",
    "        dist_i = dists[i, :]\n",
    "        #print(\"dist_i\")\n",
    "        #print(dist_i)\n",
    "        dists_sort = np.argsort(dist_i)\n",
    "        #print(\"dists_sort\")\n",
    "        #print(dists_sort)\n",
    "        indices = dists_sort[:k]\n",
    "        closest_y = []\n",
    "        #closest_y = train_y[np.where(dists_sort < k)].tolist()\n",
    "        for indx in indices:\n",
    "            y = train_y[indx]\n",
    "            closest_y+=[y]\n",
    "            #print(train_y)\n",
    "        # find most common label\n",
    "        #print(\"closest_y\")\n",
    "        #print(closest_y)\n",
    "        #y_pred[i] = max(set(closest_y), key=closest_y.count)\n",
    "        from collections import Counter\n",
    "        y_pred[i] = Counter(closest_y).most_common(1)[0][0]\n",
    "    #print(y_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data size\n",
    "print(test_x_rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to measure current data metrics\n",
    "dists = calculateDistance(test_x_rdd, train_x_rdd)\n",
    "\n",
    "print(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predictLabels(dists, train_y_rdd, 2)\n",
    "#print(y_pred)\n",
    "\n",
    "# Getting metrics\n",
    "from sklearn import metrics\n",
    "# Transform RDD\n",
    "test_y = np.array(test_y_rdd.collect())\n",
    "print(metrics.accuracy_score(test_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test..."
     ]
    }
   ],
   "source": [
    "print(\"Test...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start setting dimention to d=50\n",
      "Start generating model in PCA for k=5, d=50\n",
      "Time: 22.3\n",
      "Start applying model on data...\n",
      "Time: 1.1\n",
      "Transforming data to rdd...\n",
      "Time: 13.3\n",
      "Start calculating distance for k=5\n",
      "Time: 1060.5\n",
      "Start predicting on test data...\n",
      "Time: 1060.5\n",
      "Dimensions: 50, Knns: 5 - Accuracy: 0.96, time: 1150.5\n",
      "Start generating model in PCA for k=10, d=50\n",
      "Time: 17.4\n",
      "Start applying model on data...\n",
      "Time: 1.0\n",
      "Transforming data to rdd...\n",
      "Time: 12.6\n",
      "Start calculating distance for k=10\n",
      "Time: 1049.6\n",
      "Start predicting on test data...\n",
      "Time: 1049.6\n",
      "Dimensions: 50, Knns: 10 - Accuracy: 0.96, time: 1137.1\n",
      "Start setting dimention to d=100\n",
      "Start generating model in PCA for k=5, d=100\n",
      "Time: 16.9\n",
      "Start applying model on data...\n",
      "Time: 0.9\n",
      "Transforming data to rdd...\n",
      "Time: 15.1\n",
      "Start calculating distance for k=5\n",
      "Time: 2101.9\n",
      "Start predicting on test data...\n",
      "Time: 2101.9\n",
      "Dimensions: 100, Knns: 5 - Accuracy: 0.96, time: 2194.4\n",
      "Start generating model in PCA for k=10, d=100\n",
      "Time: 16.0\n",
      "Start applying model on data...\n",
      "Time: 1.0\n",
      "Transforming data to rdd...\n",
      "Time: 15.3\n",
      "Start calculating distance for k=10\n",
      "Time: 2090.0\n",
      "Start predicting on test data...\n",
      "Time: 2090.0\n",
      "Dimensions: 100, Knns: 10 - Accuracy: 0.96, time: 2180.8\n",
      "[0.9584, 0.9564, 0.9591, 0.9563]\n"
     ]
    }
   ],
   "source": [
    "# Run this to evaluate effect of k and d on training data\n",
    "import time\n",
    "\n",
    "from sklearn import metrics\n",
    "k_range = [5, 10]\n",
    "d_range = [50, 100]\n",
    "scores = []\n",
    "supports = []\n",
    "for d in d_range:\n",
    "    print(\"Start setting dimention to d=%d\" % d)\n",
    "    for k in k_range:\n",
    "        print(\"Start generating model in PCA for k=%d, d=%d\" % (k, d))\n",
    "        #Start Performance Timer\n",
    "        start = time.time()\n",
    "        model = generatePCA(train, d)\n",
    "        end1 = time.time()\n",
    "        print(\"Time: {0:.1f}\".format(end1-start))\n",
    "        print(\"Start applying model on data...\")\n",
    "        train_result = model.transform(train)\n",
    "        test_result = model.transform(test)\n",
    "        end2 = time.time()\n",
    "        print(\"Time: {0:.1f}\".format(end2-end1))\n",
    "        \n",
    "        print(\"Transforming data to rdd...\")\n",
    "        train_result_rdd = train_result.select([\"label\", \"pca\"]).rdd\n",
    "        test_result_rdd = test_result.select([\"label\", \"pca\"]).rdd\n",
    "\n",
    "        test_x_rdd = sc.parallelize([ row['pca'] for row in test_result_rdd.collect() ])\n",
    "        test_y_rdd = sc.parallelize([ row['label'] for row in test_result_rdd.collect() ])\n",
    "        train_x_rdd = sc.parallelize([ row['pca'] for row in train_result_rdd.collect() ])\n",
    "        train_y_rdd = sc.parallelize([ row['label'] for row in train_result_rdd.collect() ])\n",
    "        end3 = time.time()\n",
    "        print(\"Time: {0:.1f}\".format(end3-end2))\n",
    "        \n",
    "        print(\"Start calculating distance for k=%d\" % k)\n",
    "        dists = calculateDistance(test_x_rdd, train_x_rdd)\n",
    "        end4 = time.time()\n",
    "        print(\"Time: {0:.1f}\".format(end4-end3))\n",
    "        print(\"Start predicting on test data...\")\n",
    "        y_pred = predictLabels(dists, train_y_rdd, k)\n",
    "        end5 = time.time()\n",
    "        print(\"Time: {0:.1f}\".format(end4-end3))\n",
    "        end = time.time()\n",
    "\n",
    "        # Evaluate metrics\n",
    "        test_y = np.array(test_y_rdd.collect())\n",
    "        score = metrics.accuracy_score(test_y, y_pred)\n",
    "        support = metrics.precision_recall_fscore_support(test_y, y_pred)\n",
    "        supports.append(support)\n",
    "        scores.append(score)\n",
    "        print(\"Dimensions: {0}, Knns: {1} - Accuracy: {2:.2f}, time: {3:.1f}\".format(d, k,  score, end-start))\n",
    "        \n",
    "\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.master', 'spark://localhost:7077'), ('spark.app.name', 'appName'), ('spark.submit.deployMode', 'client'), ('spark.executor.instances', '8'), ('spark.ui.showConsoleProgress', 'true')]\n",
      "Help on method statusTracker in module pyspark.context:\n",
      "\n",
      "statusTracker() method of pyspark.context.SparkContext instance\n",
      "    Return :class:`StatusTracker` object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.appName\n",
    "\n",
    "sc.master\n",
    "\n",
    "print(conf.getAll())\n",
    "\n",
    "help(sc.statusTracker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "[(array([0.97386935, 0.97914857, 0.96666667, 0.9495549 , 0.96028513,\n",
      "       0.95676906, 0.95995893, 0.96192385, 0.94536082, 0.92745098]), array([0.98877551, 0.99295154, 0.95542636, 0.95049505, 0.96028513,\n",
      "       0.94282511, 0.97599165, 0.93385214, 0.94147844, 0.93756194]), array([0.98126582, 0.98600175, 0.96101365, 0.95002474, 0.96028513,\n",
      "       0.94974591, 0.9679089 , 0.94768016, 0.94341564, 0.93247905]), array([ 980, 1135, 1032, 1010,  982,  892,  958, 1028,  974, 1009])), (array([0.97085427, 0.97743056, 0.96460177, 0.95069034, 0.96197328,\n",
      "       0.94960806, 0.96197328, 0.95791583, 0.9468196 , 0.91910331]), array([0.98571429, 0.99207048, 0.9505814 , 0.95445545, 0.95315682,\n",
      "       0.95067265, 0.97703549, 0.92996109, 0.93223819, 0.9345887 ]), array([0.97822785, 0.98469611, 0.95754026, 0.95256917, 0.95754476,\n",
      "       0.95014006, 0.96944588, 0.94373149, 0.93947232, 0.92678133]), array([ 980, 1135, 1032, 1010,  982,  892,  958, 1028,  974, 1009])), (array([0.97091274, 0.98086957, 0.96390244, 0.95526839, 0.96527068,\n",
      "       0.95141243, 0.96388029, 0.95454545, 0.94129763, 0.93936382]), array([0.9877551 , 0.9938326 , 0.95736434, 0.95148515, 0.96232179,\n",
      "       0.94394619, 0.97494781, 0.93968872, 0.93839836, 0.93657086]), array([0.97926151, 0.98730853, 0.96062227, 0.95337302, 0.96379398,\n",
      "       0.9476646 , 0.96938246, 0.94705882, 0.93984576, 0.93796526]), array([ 980, 1135, 1032, 1010,  982,  892,  958, 1028,  974, 1009])), (array([0.96793587, 0.97407087, 0.96358268, 0.94876847, 0.96391753,\n",
      "       0.9452514 , 0.96296296, 0.95341923, 0.94984326, 0.9297725 ]), array([0.98571429, 0.99295154, 0.94864341, 0.95346535, 0.95213849,\n",
      "       0.94843049, 0.97703549, 0.93579767, 0.93326489, 0.93161546]), array([0.97674419, 0.98342059, 0.95605469, 0.95111111, 0.9579918 ,\n",
      "       0.94683828, 0.96994819, 0.94452626, 0.9414811 , 0.93069307]), array([ 980, 1135, 1032, 1010,  982,  892,  958, 1028,  974, 1009]))]\n"
     ]
    }
   ],
   "source": [
    "print(len(y_pred))\n",
    "print(supports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98       980\n",
      "          1       0.97      0.99      0.98      1135\n",
      "          2       0.96      0.95      0.96      1032\n",
      "          3       0.95      0.95      0.95      1010\n",
      "          4       0.96      0.95      0.96       982\n",
      "          5       0.95      0.95      0.95       892\n",
      "          6       0.96      0.98      0.97       958\n",
      "          7       0.95      0.94      0.94      1028\n",
      "          8       0.95      0.93      0.94       974\n",
      "          9       0.93      0.93      0.93      1009\n",
      "\n",
      "avg / total       0.96      0.96      0.96     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_y, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
