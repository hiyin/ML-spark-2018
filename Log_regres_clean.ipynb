{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.classification import LogisticRegression \n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml import Pipeline\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SQLContext(sc)\n",
    "data_loc = 'Data/'\n",
    "def ingest(file):\n",
    "    a = sc.read.format('csv').options(header='false', inferSchema='true').load(data_loc+file)\n",
    "    return a\n",
    "train = ingest('Train-label-28x28.csv')\n",
    "test = ingest('Test-label-28x28.csv')\n",
    "train = train.withColumnRenamed(\"_c0\",\"label\")\n",
    "test = test.withColumnRenamed(\"_c0\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions: 1, accuracy: 0.30, time: 47.8\n",
      "dimensions: 5, accuracy: 0.68, time: 34.3\n"
     ]
    }
   ],
   "source": [
    "num_dim = [1,5]\n",
    "#num_dim = [1,5,10,25,50,75,100,200,400,784]\n",
    "\n",
    "for x in num_dim:\n",
    "    #Change pipeline\n",
    "    transformer = VectorAssembler(inputCols=['_c%d' % i for i in range(1,785)],\n",
    "                                  outputCol=\"features\")\n",
    "    standardizer = StandardScaler(withMean=True, withStd=True,\n",
    "                                  inputCol='features',\n",
    "                                  outputCol='std_features')\n",
    "    indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_idx\")\n",
    "    pca = PCA(k=x, inputCol=\"std_features\", outputCol=\"pca\")\n",
    "    lr = LogisticRegression(featuresCol='pca', labelCol='label_idx')\n",
    "    pipeline = Pipeline(stages=[transformer, standardizer, indexer, pca, lr])\n",
    "    \n",
    "    #Start Performance Timer\n",
    "    start = time.time()\n",
    "    \n",
    "    #Retrain\n",
    "    model = pipeline.fit(train)\n",
    "    \n",
    "    #test\n",
    "    prediction = model.transform(test)\n",
    "    \n",
    "    #Stop performance timer (time of train & test)\n",
    "    end = time.time()\n",
    "    \n",
    "    #Score\n",
    "    score = prediction.select(['label_idx', 'prediction'])\n",
    "    \n",
    "    #final score\n",
    "    acc = score.rdd.map(lambda x: x[0] == x[1]).sum() / score.count()\n",
    "    print('dimensions: {0}, accuracy: {1:.2f}, time: {2:.1f}'.format(x,acc,end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Spark - local",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
